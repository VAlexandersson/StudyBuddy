{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'P',\n",
       "  'text': 'The pace at which computer systems change was, is, and continues to be overwhelming. From 1945, when the modern computer era began, until about 1985, computers were large and expensive. Moreover, lacking a way to connect them, these computers operated independently of one another. ',\n",
       "  'chapter': '01 INTRODUCTION ',\n",
       "  'parent_chapter': nan,\n",
       "  'char_count': 282,\n",
       "  'word_count': 44,\n",
       "  'sentence_count': 3,\n",
       "  'token_count': 70.5},\n",
       " {'type': 'P[2]',\n",
       "  'text': 'Starting in the mid-1980s, however, two advances in technology began to change that situation. The first was the development of powerful microprocessors. Initially, these were 8-bit machines, but soon 16-, 32-, and 64-bit CPUs became common. With powerful multicore CPUs, we now are again facing the challenge of adapting and developing programs to exploit parallelism. In any case, the current generation of machines have the computing power of the mainframes deployed 30 or 40 years ago, but for 1/1000th of the price or less. ',\n",
       "  'chapter': '01 INTRODUCTION ',\n",
       "  'parent_chapter': nan,\n",
       "  'char_count': 529,\n",
       "  'word_count': 84,\n",
       "  'sentence_count': 5,\n",
       "  'token_count': 132.25}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text_chunks_path = './data/text_chunks.csv'\n",
    "text_chunks_df = pd.read_csv(text_chunks_path, index_col=0)\n",
    "text_chunks_dict = text_chunks_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model_id = \"all-mpnet-base-v2\"\n",
    "embedding_model = SentenceTransformer(model_name_or_path=embedding_model_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import copy \n",
    "# text_chunks_dict_copy = copy.deepcopy(text_chunks_dict)\n",
    "# from tqdm.auto import tqdm\n",
    "# for item in tqdm(text_chunks_dict_copy):\n",
    "#     item[\"embedding\"] = embedding_model.encode(item[\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_list = [item[\"text\"] for item in text_chunks_dict]\n",
    "# Embed texts in batches\n",
    "text_chunks_embeddings =embedding_model.encode(text_list, batch_size=32, convert_to_tensor=False, show_progress_bar=True)\n",
    "for i, item in enumerate(text_chunks_dict):\n",
    "    item[\"embedding\"] = text_chunks_embeddings[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save chunks w/ embeddings as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_with_embeddings_df = pd.DataFrame(text_chunks_dict)\n",
    "chunks_with_embeddings_path = \"./data/text_chunks_with_embeddings.csv\"\n",
    "text_chunks_with_embeddings_df.to_csv(chunks_with_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSV.\n",
    "\n",
    "Load CSV with chunks and its embedding.\n",
    "\n",
    "Process the CSV into right format.\n",
    "\n",
    "Create the embedding model.\n",
    "\n",
    "Create a search pipeline for the query and the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2898, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "from helpers import import_chunks_with_embeddings, get_chunks_embeddings_as_tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "csv_path = \"./data/text_chunks_with_embeddings.csv\"\n",
    "\n",
    "chunks_with_embeddings = import_chunks_with_embeddings(csv_path)\n",
    "\n",
    "embeddings = get_chunks_embeddings_as_tensor(chunks_with_embeddings).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to compute dot scores on (2898): 8.282496128231287e-05 seconds\n",
      "Score: 0.5530380010604858\n",
      "Chapter: Object-based architectural style\n",
      "Text:\n",
      "The server-side stub is often referred to as a skeleton as it provides the bare\n",
      "means for letting the server middleware access the user-defined objects. In\n",
      "practice, it often contains incomplete code in the form of a language-specific\n",
      "class that needs to be further specialized by the developer.\n",
      "\n",
      "\n",
      "Score: 0.5495573878288269\n",
      "Chapter: 4.2.2 Parameter passing\n",
      "Text:\n",
      "The function of the client stub is to take its parameters, pack them into a\n",
      "message, and send them to the server stub. While this sounds straightforward, it\n",
      "is not quite as simple as it at first appears.\n",
      "\n",
      "\n",
      "Score: 0.4721587896347046\n",
      "Chapter: Note 4.8 (Advanced: Implementing stubs as global references revisited)\n",
      "To provide a more in-depth insight in the working of sockets, let us look at a\n",
      "more elaborate example, namely the use of stubs as global references.\n",
      "Text:\n",
      "To use a stub as a global reference, we represent each client application by the\n",
      "class Client shown in Figure (<>)4.20(c). The class is instantiated in the same\n",
      "process running the application (exemplified by the value of self.host), and\n",
      "will be listening on a specific port for messages from other applications, as\n",
      "well as the server. Otherwise, it merely sends and receives messages, coded\n",
      "through the operations sendTo and recvAny, respectively.\n",
      "\n",
      "\n",
      "Score: 0.4465748369693756\n",
      "Chapter: 4.2.1 Basic RPC operation\n",
      "Text:\n",
      "When the server stub gets control back after the call has completed, it packs\n",
      "the result in a message and calls send to return it to the client. Thereafter,\n",
      "the server stub usually does a call to receive again, to wait for the next\n",
      "incoming request.\n",
      "\n",
      "\n",
      "Score: 0.40183162689208984\n",
      "Chapter: AMQP communication\n",
      "Text:\n",
      "1. At the sender’s side, the message is assigned a unique identifier and is\n",
      "recorded locally to be in an unsettled state. The stub subsequently transfers\n",
      "the message to the server, where the AMQP stub also records it as being in an\n",
      "unsettled state. At that point, the server-side stub passes it to the queue\n",
      "manager. 2. The receiving application (in this case the queue manager), is\n",
      "assumed to handle the message and normally reports back to its stub that it is\n",
      "finished. The stub passes this information to the original sender, at which\n",
      "point the message at the original sender’s AMQP stub enters a settled state. 3.\n",
      "The AMQP stub of the original sender now tells the stub of the original receiver\n",
      "that message transfer has been settled (meaning that the original sender will\n",
      "forget about the message from now on). The receiver’s stub can now also discard\n",
      "anything about the message, formally recording it as being settled as well.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import retrieve_relevant_resources, print_top_results_and_scores\n",
    "\n",
    "query = \"What is a STUB?\"\n",
    "\n",
    "print_top_results_and_scores(\n",
    "    *retrieve_relevant_resources(query=query, \n",
    "                                 embeddings=embeddings, \n",
    "                                 embedding_model=embedding_model),\n",
    "    chunks_with_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total     GPU memory: 24 GB\n",
      "Available GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import resource_utils\n",
    "importlib.reload(resource_utils)\n",
    "from resource_utils import *\n",
    "\n",
    "print(f\"Total     GPU memory: {get_total_gpu_memory()} GB\")\n",
    "print(f\"Available GPU memory: {round(get_available_vram()[0]/1024, 1)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sdpa attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adc7514d9b94612996d1174f382aa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Attention implementation, either 'sdpa' or 'flash_attention_2'\n",
    "if(is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print(f\"Using {attn_implementation} attention\")\n",
    "\n",
    "# Model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Instantiate tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "model = llm_model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8030261248\n",
      "{'model_mem_bytes': 16194748416, 'model_mem_mb': 15444.52, 'model_mem_gb': 15.08}\n"
     ]
    }
   ],
   "source": [
    "print(get_model_num_params(llm_model))\n",
    "print(get_model_mem_size(llm_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text with Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: What are the fundamental principles of Named Data Networking (NDN) and how does it differ from traditional host-based networking?\n",
      "\n",
      "Prompt formatted:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Study-Buddy. An educatinal chatbot that will aid students in their studies.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the fundamental principles of Named Data Networking (NDN) and how does it differ from traditional host-based networking?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the fundamental principles of Named Data Networking (NDN) and how does it differ from traditional host-based networking?\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# Prompt template\n",
    "message_template = [\n",
    "    { \"role\": \"system\", \"content\": \"You are Study-Buddy. An educatinal chatbot that will aid students in their studies.\" },\n",
    "    { \"role\": \"user\", \"content\": input_text }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    message_template,\n",
    "    add_generation_prompt=True,\n",
    "    #return_tensors=\"pt\"#\n",
    "    tokenize=False # keep as raw text\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt formatted:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 14.3 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize the input text (turn it into numbers) and sen iut to the GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = llm_model.generate(**input_ids, max_new_tokens=500)\n",
    "\n",
    "# print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Study-Buddy. An educatinal chatbot that will aid students in their studies.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the fundamental principles of Named Data Networking (NDN) and how does it differ from traditional host-based networking?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'd be happy to help you with that!\n",
      "\n",
      "Named Data Networking (NDN) is a new networking paradigm that focuses on naming and retrieving data instead of addressing and routing packets. It's a significant departure from traditional host-based networking, which focuses on addressing and routing packets between hosts.\n",
      "\n",
      "Here are the fundamental principles of NDN:\n",
      "\n",
      "1. **Named Data**: In NDN, data is identified by its name, rather than its location or sender. This allows for more efficient and flexible data retrieval, as data can be retrieved directly by its name, without knowing its source or location.\n",
      "2. **Content-Centric**: NDN is a content-centric network, meaning that data is the central focus, rather than the communication between hosts. This allows for more efficient and scalable data retrieval, as well as improved security and reliability.\n",
      "3. **Data-Centric Routing**: NDN uses a data-centric routing approach, where packets are routed based on the data's name, rather than the sender's address. This allows for more efficient and flexible routing, as well as improved security and reliability.\n",
      "4. **In-Network Caching**: NDN uses in-network caching, where intermediate nodes in the network cache and store data, allowing for faster and more efficient data retrieval.\n",
      "5. **Interest-Data Pair**: In NDN, data is retrieved by sending an \"interest\" message, which is a request for the data, and the data is returned in response to the interest.\n",
      "\n",
      "Now, let's compare NDN to traditional host-based networking:\n",
      "\n",
      "1. **Addressing**: In traditional host-based networking, hosts are addressed using IP addresses, whereas in NDN, data is addressed using names.\n",
      "2. **Packet Routing**: Traditional host-based networking uses packet routing, where packets are routed based on the sender's address and the destination's address. NDN uses data-centric routing, where packets are routed based on the data's name.\n",
      "3. **Data Retrieval**: In traditional host-based networking, data is retrieved by sending a request to the host that has the data, whereas in NDN, data is retrieved by sending an interest message, which is a request for the data.\n",
      "4. **Scalability**: NDN is designed to be more scalable than traditional host-based networking, as it allows for more efficient and flexible data retrieval, as well as improved security and reliability.\n",
      "5. **Security**: NDN provides improved security, as data is retrieved by its name, rather than its location or\n"
     ]
    }
   ],
   "source": [
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('questions.md', 'r') as file:\n",
    "    query_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the advantages and disadvantages of using threads compared to processes in a distributed system?\n",
      "Time taken to compute dot scores on (2898): 9.622599463909864e-05 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7697, 0.6948, 0.6759, 0.6685, 0.6663], device='cuda:0'),\n",
       " tensor([554, 505, 534, 488, 494], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from helpers import retrieve_relevant_resources\n",
    "\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, embedding_model=embedding_model)\n",
    "\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting prompt with context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7800]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are Study-Buddy. An educational chatbot that will aid students in their studies.\n",
    "You are given the extracted parts of curriulum specific documents and a question. Provide a conversational and educational answer with good and easily read formatting.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "def get_user_prompt(query: str, retreived_documents: list[dict]):\n",
    "    \"\"\"\n",
    "    Formats the prompt with the query and the retreived documents.\n",
    "    \"\"\"\n",
    "    base_prompt = f\"Query: {query}\\nContext:\"\n",
    "    for item in retreived_documents:\n",
    "        base_prompt += f\"\\n- {item['text']}\"\n",
    "    # base_prompt += [item[\"text\"] for item in retreived_documents]\n",
    "    \n",
    "    # context_items = [item for i, item in enumerate(chunks_with_embeddings) if i in indices]\n",
    "    # prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "    return base_prompt\n",
    "\n",
    "def format_prompt(formatted_prompt: str):\n",
    "    message = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT },\n",
    "        { \"role\": \"user\", \"content\": formatted_prompt }\n",
    "    ]\n",
    "    return message\n",
    "    \n",
    "def generate_response(prompt: str):\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=1024, \n",
    "        eos_token_id = terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "def study_buddy(query: str):\n",
    "    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, embedding_model=embedding_model, print_time=False)\n",
    "    user_prompt = get_user_prompt(query=query, retreived_documents=[chunks_with_embeddings[i] for i in indices])\n",
    "    formatted_prompt = format_prompt(user_prompt) \n",
    "    return generate_response(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------\n",
      "Query:\n",
      "\n",
      "What are the main components of a Content Delivery Network (CDN) and how do they\n",
      "work together to improve content delivery performance?\n",
      "\n",
      "-----------------\n",
      "\n",
      "The main components of a Content Delivery Network (CDN) are:\n",
      "\n",
      "1. **Origin Server**: This is the primary server that hosts the content of a website. It is responsible for storing and serving the original content.\n",
      "2. **Edge Servers**: These are strategically located servers that are part of the CDN. They are responsible for caching and serving content to users. Edge servers are typically located closer to the users, which reduces latency and improves performance.\n",
      "3. **Content Cache**: This is a storage mechanism that stores copies of content on edge servers. The content cache ensures that the selected edge server has the required content readily available.\n",
      "4. **Content Replication**: This is the process of copying content from the origin server to multiple edge servers. Content replication ensures that the content is available across multiple locations, improving availability and reducing latency.\n",
      "5. **Content Update Mechanism**: This is a mechanism that ensures that the content on edge servers is updated when changes are made to the original content on the origin server.\n",
      "\n",
      "These components work together to improve content delivery performance by:\n",
      "\n",
      "* Reducing latency: By caching content on edge servers, users can access content faster.\n",
      "* Improving availability: Content replication ensures that content is available across multiple locations, reducing the risk of downtime.\n",
      "* Reducing bandwidth usage: Content caching and replication reduce the amount of bandwidth required to deliver content.\n",
      "* Improving scalability: CDNs can handle large amounts of traffic by distributing content across multiple edge servers.\n",
      "\n",
      "Overall, the combination of these components enables CDNs to provide fast, reliable, and scalable content delivery services to users worldwide.\n"
     ]
    }
   ],
   "source": [
    "from helpers import print_wrapped\n",
    "query = random.choice(query_list)\n",
    "print(\"\\n-----------------\\nQuery:\\n\")\n",
    "print_wrapped(f\"{query}\")\n",
    "print(\"\\n-----------------\\n\")\n",
    "print(study_buddy(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments the query with the context items.\n",
    "    \"\"\"\n",
    "    context = \"- \" + \"\\n- \".join([item[\"text\"] for item in context_items])\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query. \n",
    "    Do not mention any refrences or sources in your answer.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your your answers are as explanatory and educational as possible.\n",
    "    Now use the following context items to answer the query:\n",
    "    Context items:\n",
    "    {context}\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    \n",
    "    message_template = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT },\n",
    "        { \"role\": \"user\", \"content\": input_text }\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "    message_template,\n",
    "    add_generation_prompt=True,\n",
    "    #return_tensors=\"pt\"#\n",
    "    tokenize=False # keep as raw text\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the challenges of achieving both consistency and fault tolerance in large-scale distributed systems?\n",
      "Time taken to compute dot scores on (2898): 8.242693729698658e-05 seconds\n",
      "\n",
      "\n",
      "Prompt:\n",
      "Based on the following context items, please answer the query. \n",
      "    Do not mention any refrences or sources in your answer.\n",
      "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
      "    Don't return the thinking, only return the answer.\n",
      "    Make sure your your answers are as explanatory and educational as possible.\n",
      "    Now use the following context items to answer the query:\n",
      "    Context items:\n",
      "    - To understand the role of fault tolerance in distributed systems, we first need to take a closer look at what it actually means for a distributed system to tolerate faults. Being fault tolerant is strongly related to what are called dependable systems. Dependability is a term that covers a number of useful requirements for distributed systems, including the following [(<>)Kopetz and (<>)Verissimo, (<>)1993]: \n",
      "- Now that the basic issues of fault tolerance have been discussed, let us concentrate on how fault tolerance can actually be achieved in distributed systems. The first topic we discuss is protection against process failures, which is achieved by replicating processes into groups. In the following pages, we consider the general design issues of process groups and discuss what a fault-tolerant group actually is. Also, we look at how to reach consensus within a process group when one or more of its members cannot be trusted to give correct answers. \n",
      "- As its name suggests, dependability refers to the degree that a computer system can be relied upon to operate as expected. In contrast to single-computer systems, dependability in distributed systems can be rather intricate due to partial failures: somewhere there is a component failing while the system as a whole still seems to be living up to expectations (up to a certain point or moment). Although single-computer systems can also suffer from failures that do not appear immediately, having a potentially large collection of networked computer systems complicates matters considerably. In fact, one should assume that at any time, there are always partial failures occurring. An important goal of distributed systems is to mask those failures, as well as mask the recovery from those failures. This masking is the essence of being able to tolerate faults, accordingly referred to as fault tolerance. \n",
      "- Organizing replicated processes into a group helps to increase fault tolerance. However, what should have become clear by now is that there is a price to pay, namely a potential loss of performance. In the solutions discussed so far, processes in a fault-tolerant group may need to exchange numerous messages before reaching a decision. The Byzantine agreement protocol is an excellent illustration of how tightly coupled processes may be. The question that comes to mind is whether realizing specific forms of fault tolerance, like being able to withstand arbitrary failures, is always possible. \n",
      "- • There are many, often unexpected, dependencies that hinder understanding the behavior of these systems. • Distributed and decentralized systems suffer almost continuously from partial failures: some process or resource, somewhere at one of the participating computers, is not operating according to expectations. Discovering that failure may actually take some time, while also such failures are preferably masked (i.e., they go unnoticed for users and applications), including the recovery from failures. \n",
      "    Query: What are the challenges of achieving both consistency and fault tolerance in large-scale distributed systems?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, embedding_model=embedding_model)\n",
    "\n",
    "context_items = [chunks_with_embeddings[i] for i in indices]\n",
    "prompt = prompt_formatter(query, context_items)\n",
    "\n",
    "print(f\"\\n\\nPrompt:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:What are the challenges of achieving both consistency and fault tolerance in large-scale distributed systems?\n",
      "RAG answer:\n",
      "<|begin_of_text|> Based on the context items provided, what are the challenges of achieving both consistency and fault tolerance in large-scale distributed systems?\n",
      "\n",
      "Answer: \n",
      "The main challenges of achieving both consistency and fault tolerance in large-scale distributed systems are the potential loss of performance and the complexity of masking partial failures. In order to achieve fault tolerance, processes in a fault-tolerant group may need to exchange numerous messages, which can lead to a decrease in performance. Additionally, the complexity of masking partial failures and the recovery from those failures can be intricate, especially in large-scale distributed systems where there are many dependencies and unexpected failures can occur. Moreover, realizing specific forms of fault tolerance, such as being able to withstand arbitrary failures, may not always be possible. Therefore, achieving both consistency and fault tolerance in large-scale distributed systems requires careful consideration of these challenges and the development of solutions that balance performance, fault tolerance, and dependability. <|eot_id|>\n",
      "CPU times: user 3.8 s, sys: 23.9 ms, total: 3.83 s\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = llm_model.generate(**input_ids, \n",
    "                             temperature=0.7,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=500)\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f\"Query:{query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt,'')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
