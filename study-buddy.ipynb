{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSV.\n",
    "\n",
    "Load CSV with chunks and its embedding.\n",
    "\n",
    "Process the CSV into right format.\n",
    "\n",
    "Create the embedding model.\n",
    "\n",
    "Create a search pipeline for the query and the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2898, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "from helpers import import_chunks_with_embeddings, get_chunks_embeddings_as_tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "csv_path = \"./data/filtered_all_chapters_embeddings_df.csv\"\n",
    "\n",
    "\n",
    "\n",
    "chunks_with_embeddings = import_chunks_with_embeddings(csv_path)\n",
    "\n",
    "embeddings = get_chunks_embeddings_as_tensor(chunks_with_embeddings).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to compute dot scores on (2898): 8.900801185518503e-05 seconds\n",
      "Score: 0.5530380010604858\n",
      "Chapter: Object-based architectural style\n",
      "Text:\n",
      "The server-side stub is often referred to as a skeleton as it provides the bare\n",
      "means for letting the server middleware access the user-defined objects. In\n",
      "practice, it often contains incomplete code in the form of a language-specific\n",
      "class that needs to be further specialized by the developer.\n",
      "\n",
      "\n",
      "Score: 0.5495573282241821\n",
      "Chapter: 4.2.2 Parameter passing\n",
      "Text:\n",
      "The function of the client stub is to take its parameters, pack them into a\n",
      "message, and send them to the server stub. While this sounds straightforward, it\n",
      "is not quite as simple as it at first appears.\n",
      "\n",
      "\n",
      "Score: 0.47215867042541504\n",
      "Chapter: Note 4.8 (Advanced: Implementing stubs as global references revisited)\n",
      "To provide a more in-depth insight in the working of sockets, let us look at a\n",
      "more elaborate example, namely the use of stubs as global references.\n",
      "Text:\n",
      "To use a stub as a global reference, we represent each client application by the\n",
      "class Client shown in Figure (<>)4.20(c). The class is instantiated in the same\n",
      "process running the application (exemplified by the value of self.host), and\n",
      "will be listening on a specific port for messages from other applications, as\n",
      "well as the server. Otherwise, it merely sends and receives messages, coded\n",
      "through the operations sendTo and recvAny, respectively.\n",
      "\n",
      "\n",
      "Score: 0.44657477736473083\n",
      "Chapter: 4.2.1 Basic RPC operation\n",
      "Text:\n",
      "When the server stub gets control back after the call has completed, it packs\n",
      "the result in a message and calls send to return it to the client. Thereafter,\n",
      "the server stub usually does a call to receive again, to wait for the next\n",
      "incoming request.\n",
      "\n",
      "\n",
      "Score: 0.4018314778804779\n",
      "Chapter: AMQP communication\n",
      "Text:\n",
      "1. At the sender’s side, the message is assigned a unique identifier and is\n",
      "recorded locally to be in an unsettled state. The stub subsequently transfers\n",
      "the message to the server, where the AMQP stub also records it as being in an\n",
      "unsettled state. At that point, the server-side stub passes it to the queue\n",
      "manager. 2. The receiving application (in this case the queue manager), is\n",
      "assumed to handle the message and normally reports back to its stub that it is\n",
      "finished. The stub passes this information to the original sender, at which\n",
      "point the message at the original sender’s AMQP stub enters a settled state. 3.\n",
      "The AMQP stub of the original sender now tells the stub of the original receiver\n",
      "that message transfer has been settled (meaning that the original sender will\n",
      "forget about the message from now on). The receiver’s stub can now also discard\n",
      "anything about the message, formally recording it as being settled as well.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import retrieve_relevant_resources, print_top_results_and_scores\n",
    "\n",
    "query = \"What is a STUB?\"\n",
    "\n",
    "print_top_results_and_scores(\n",
    "    *retrieve_relevant_resources(query=query, \n",
    "                                 embeddings=embeddings, \n",
    "                                 embedding_model=embedding_model),\n",
    "    chunks_with_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 24 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sdpa attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d606b22a7f140eea3aa358b865c5ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Attention implementation, either 'sdpa' or 'flash_attention_2'\n",
    "if(is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print(f\"Using {attn_implementation} attention\")\n",
    "\n",
    "# Model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Instantiate tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "llm_model = llm_model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 21:10:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   42C    P2             70W /  450W |   16409MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1465      G   /usr/lib/xorg/Xorg                              9MiB |\n",
      "|    0   N/A  N/A      1875      G   /usr/bin/gnome-shell                           10MiB |\n",
      "|    0   N/A  N/A     26693      C   /home/buddy/Study-Buddy/env/bin/python      16372MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8030261248"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 16194748416,\n",
       " 'model_mem_mb': 15444.52,\n",
       " 'model_mem_gb': 15.08}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text with Llama 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: What is the meaning of process and thread?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is the meaning of process and thread?\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# Prompt template\n",
    "message_template = [\n",
    "    { \"role\": \"system\", \"content\": \"You are Study-Buddy. An educatinal chatbot that will aid students in their studies.\" },\n",
    "    { \"role\": \"user\", \"content\": input_text }\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    message_template,\n",
    "    tokenize=False, # keep as raw text\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt formatted:\\n{input_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt formatted:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Study-Buddy. An educatinal that will aid students in their studies.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the meaning of process and thread?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt formatted:\\n{input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\n\u001b[1;32m      2\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = llm_model\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
