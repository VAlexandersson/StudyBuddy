{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCCESSES:  []\n",
      "TOTAL VRAM:  23.988\n",
      "AVAILABLE VRAM:  23.613\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import resource_utils\n",
    "importlib.reload(resource_utils)\n",
    "from resource_utils import get_total_vram, get_available_vram, get_gpu_processes_usage, kill_gpu_processes\n",
    "\n",
    "print(f\"PROCCESSES: \", get_gpu_processes_usage())\n",
    "print(\"TOTAL VRAM: \", get_total_vram())\n",
    "print(\"AVAILABLE VRAM: \", get_available_vram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_gpu_processes(get_gpu_processes_usage(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "with open('questions.md', 'r') as file:\n",
    "    query_list = file.read().splitlines()\n",
    "\n",
    "CONFIG = {\n",
    "    'csv_path': './data/text_chunks_with_embeddings.csv',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'attn_implementation': 'sdpa',\n",
    "    'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'embedding_model_id': 'all-mpnet-base-v2',\n",
    "}\n",
    "\n",
    "SYS_PROMPT = {\n",
    "    \"education\": \"\"\"\n",
    "    You are Study-Buddy. An educational chatbot that will aid students in their studies.\n",
    "    You are given the extracted parts of curriculum specific documents and a question. Provide a conversational and educational answer with good and easily read formatting.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
    "    \"\"\",\n",
    "    \"relevance\": \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "    \"\"\",\n",
    "    \"socratic_sage\": \"\"\"\n",
    "    You are an AI assistant capable of having in-depth Socratic style conversations on a wide range of topics. Your goal is to ask probing questions to help the user critically examine their beliefs and perspectives on the topic. Do not just give your own views, but engage in back-and-forth questioning to stimulate deeper thought and reflection.\n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def import_chunks_with_embeddings(csv_path: str):\n",
    "    \"\"\"\n",
    "    Imports the chunks with embeddings from a csv file.\n",
    "    \"\"\"\n",
    "    text_chunks_with_embeddings_df = pd.read_csv(csv_path, index_col=0)\n",
    "    text_chunks_with_embeddings_df['embedding'] = text_chunks_with_embeddings_df['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "    chunks_with_embeddings = text_chunks_with_embeddings_df.to_dict(orient='records')\n",
    "    return chunks_with_embeddings\n",
    "\n",
    "def get_chunks_embeddings_as_tensor(chunks_with_embeddings: list[dict]):\n",
    "    \"\"\"\n",
    "    Converts the embeddings of chunks to a tensor.\n",
    "    \"\"\"\n",
    "    embeddings_list = [chunk['embedding'] for chunk in chunks_with_embeddings]\n",
    "    embeddings = torch.tensor(np.stack(embeddings_list, axis=0), dtype=torch.float32)\n",
    "    # embeddings = torch.tensor(np.stack(chunks_with_embeddings['embedding'].tolist(), axis=0), dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "# Load chunks and embeddings\n",
    "chunks_with_embeddings = import_chunks_with_embeddings(CONFIG['csv_path'])\n",
    "embeddings = get_chunks_embeddings_as_tensor(chunks_with_embeddings).to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Inference\n",
    "\n",
    "- Using Llama 3: https://huggingface.co/blog/llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                embedding_model: SentenceTransformer,\n",
    "                                n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Get dot product scores on embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "    \n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def generate_model_response(prompt: str, tokenizer, model, terminators, device=\"cuda\"):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=1024, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Generated response at outputs[0] but starting at position input_ids.shape[-1]. \n",
    "    # [input_ids.shape[-1]:] is done to remove the input tokens and only keep the generated text.\n",
    "    response = outputs[0][input_ids.shape[-1]:] \n",
    "    return tokenizer.decode(response)\n",
    "\n",
    "\n",
    "def get_user_prompt(query: str, retrieved_documents: list[dict]):\n",
    "    \"\"\"\n",
    "    Formats the prompt with the query and the retreived documents.\n",
    "    \"\"\"\n",
    "    base_prompt = f\"Query: {query}\\nContext:\"\n",
    "    for item in retrieved_documents:\n",
    "        base_prompt += f\"\\n- {item['text']}\"\n",
    "    return base_prompt\n",
    "\n",
    "def format_prompt(formatted_prompt: str, sys_prompt: str):\n",
    "    message = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT[sys_prompt] },\n",
    "        { \"role\": \"user\", \"content\": formatted_prompt }\n",
    "    ]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "embedding_model = SentenceTransformer(model_name_or_path=CONFIG['embedding_model_id'], device=CONFIG['device'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=CONFIG['model_id'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=CONFIG['model_id'], \n",
    "    torch_dtype=torch.float16, \n",
    "    #low_cpu_mem_usage=False, \n",
    "    attn_implementation=CONFIG['attn_implementation']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "----\n",
      "Achieving full distribution transparency is often impractical or even undesirable because it may not always be possible to hide all distribution aspects from users. In some cases, making distribution explicit can be beneficial, as it allows users and application developers to understand the sometimes unexpected behavior of a distributed system and be better prepared to deal with it. Additionally, hiding distribution can lead to poorly understood semantics, and may not be necessary or desirable in all situations, such as with location-based services that rely on exposing distribution.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "\n",
    "# similarity scores and indices on chunk embeddings\n",
    "scores, indices = retrieve_relevant_resources(\n",
    "    query=query, \n",
    "    embeddings=embeddings, \n",
    "    embedding_model=embedding_model)\n",
    "\n",
    "user_prompt = get_user_prompt(query=query, retrieved_documents=[chunks_with_embeddings[i] for i in indices])\n",
    "\n",
    "formatted_prompt = format_prompt(user_prompt, \"education\") \n",
    "\n",
    "response = generate_model_response(formatted_prompt, tokenizer, model, terminators)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Contextual Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Why is achieving full distribution transparency often impractical or even undesirable?\"\n",
    "scores, indices = retrieve_relevant_resources(\n",
    "    query=q, \n",
    "    embeddings=embeddings, \n",
    "    embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
      "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
      "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Query: Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "Retrieved Document: Although distribution transparency is generally considered preferable for any distributed system, there are situations in which blindly attempting to hide all distribution aspects from users is not a good idea. A simple example is requesting your electronic newspaper to appear in your mailbox before 7 AM local time, as usual, while you are currently at the other end of the world living in a different time zone. Your morning paper will not be the morning paper you are used to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "torch.Size([1, 221])\n"
     ]
    }
   ],
   "source": [
    "from helpers import print_top_results_and_scores\n",
    "\n",
    "def grade_retreival(query: str, retrieved_document: str, verbose: bool = False, temperature: float = 0.6, top_p: float =0.9):\n",
    "    \"\"\"\n",
    "    Grades the retrieval of documents based on the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adds retrieved documents to the prompt\n",
    "    user_prompt = f\"Query: {query}\\nRetrieved Document: {retrieved_document}\"\n",
    "    \n",
    "    # Format prompt with system info and user query\n",
    "    message = format_prompt(user_prompt, \"relevance\")\n",
    "    \n",
    "    #message = [\n",
    "    #    { \"role\": \"system\", \"content\": SYS_PROMPT['relevance'] },\n",
    "    #    { \"role\": \"user\", \"content\": base_prompt }\n",
    "    #]\n",
    "    \n",
    "    #  Apply chat template to prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True #, \n",
    "        # return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize prompt ( can be done in previous step with return_tensors=\"pt\" and tokenize=True )\n",
    "    input_ids =  tokenizer(prompt, return_tensors=\"pt\").to(CONFIG['device'])[\"input_ids\"]\n",
    "    if verbose:\n",
    "        print(prompt)\n",
    "        #print(input_ids)\n",
    "        print(input_ids.shape)\n",
    "    # Generate response, gets it decoded.\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=256, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode response\n",
    "    outputs = outputs[0][input_ids.shape[-1]:]\n",
    "    return outputs\n",
    "\n",
    "retrieved_documents = [chunks_with_embeddings[i] for i in indices]\n",
    "grad = grade_retreival(q, retrieved_documents[0][\"text\"], True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "--\n",
      "Although distribution transparency is generally considered preferable for any\n",
      "distributed system, there are situations in which blindly attempting to hide all\n",
      "distribution aspects from users is not a good idea. A simple example is\n",
      "requesting your electronic newspaper to appear in your mailbox before 7 AM local\n",
      "time, as usual, while you are currently at the other end of the world living in\n",
      "a different time zone. Your morning paper will not be the morning paper you are\n",
      "used to.\n",
      "{\"score\": \"yes\"}\n"
     ]
    }
   ],
   "source": [
    "from helpers import print_wrapped\n",
    "print(f\"Q: {q}\\n--\")\n",
    "print_wrapped(retrieved_documents[0]['text'])\n",
    "output_text = tokenizer.decode(grad, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessment 1: {\"score\": \"no\"}\n",
      "assessment 2: {\"score\": \"yes\"}\n",
      "assessment 3: {\"score\": \"yes\"}\n",
      "assessment 4: {\"score\": \"no\"}\n",
      "assessment 5: {\"score\": \"yes\"}\n"
     ]
    }
   ],
   "source": [
    "for i, document in enumerate(retrieved_documents):\n",
    "    grad = grade_retreival(q, document['text'])\n",
    "    output_text = tokenizer.decode(grad, skip_special_tokens=True)\n",
    "    print(f\"assessment {i+1}: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_documents(temperature = 0.6, top_p = 0.9):\n",
    "    for i, document in enumerate(retrieved_documents):\n",
    "        grad = grade_retreival(q, document['text'], False, temperature, top_p)\n",
    "        output_text = tokenizer.decode(grad, skip_special_tokens=True)\n",
    "        print(f\"assessment {i+1}: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessment 1: {\"score\": \"yes\"}\n",
      "assessment 2: {\"score\": \"yes\"}\n",
      "assessment 3: {\"score\": \"yes\"}\n",
      "assessment 4: {\"score\": \"no\"}\n",
      "assessment 5: {\"score\": \"no\"}\n"
     ]
    }
   ],
   "source": [
    "assess_documents(temperature=0.6, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessment 1: {\"score\": \"no\"}\n",
      "assessment 2: {\"score\": \"yes\"}\n",
      "assessment 3: {\"score\": \"yes\"}\n",
      "assessment 4: {\"score\": \"no\"}\n",
      "assessment 5: {\"score\": \"yes\"}\n"
     ]
    }
   ],
   "source": [
    "assess_documents(temperature=0.1, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_assessment_1>\n",
      "QUERY:\n",
      "Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "RETRIEVED TEXT:\n",
      "Although distribution transparency is generally considered preferable for any\n",
      "distributed system, there are situations in which blindly attempting to hide all\n",
      "distribution aspects from users is not a good idea. A simple example is\n",
      "requesting your electronic newspaper to appear in your mailbox before 7 AM local\n",
      "time, as usual, while you are currently at the other end of the world living in\n",
      "a different time zone. Your morning paper will not be the morning paper you are\n",
      "used to.\n",
      "RELEVANCE:\n",
      "{\"score\": \"no\"}\n",
      "<end_assessment_1>\n",
      "\n",
      "<start_assessment_2>\n",
      "QUERY:\n",
      "Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "RETRIEVED TEXT:\n",
      "There are other arguments against distribution transparency. Recognizing that\n",
      "full distribution transparency is simply impossible, we should ask ourselves\n",
      "whether it is even wise to pretend that we can achieve it. It may be much better\n",
      "to make distribution explicit so that the user and application developer are\n",
      "never tricked into believing that there is such a thing as transparency. The\n",
      "result will be that users will much better understand the (sometimes unexpected)\n",
      "behavior of a distributed system, and are thus much better prepared to deal with\n",
      "this behavior.\n",
      "RELEVANCE:\n",
      "{\"score\": \"yes\"}\n",
      "<end_assessment_2>\n",
      "\n",
      "<start_assessment_3>\n",
      "QUERY:\n",
      "Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "RETRIEVED TEXT:\n",
      "The conclusion is that aiming for distribution transparency may be a nice goal\n",
      "when designing and implementing distributed systems, but that it should be\n",
      "considered together with other issues such as performance and comprehensibility.\n",
      "The price for achieving full transparency may be surprisingly high.\n",
      "RELEVANCE:\n",
      "{\"score\": \"yes\"}\n",
      "<end_assessment_3>\n",
      "\n",
      "<start_assessment_4>\n",
      "QUERY:\n",
      "Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "RETRIEVED TEXT:\n",
      "Finally, there are situations in which it is not at all obvious that hiding\n",
      "distribution is a good idea. As distributed systems are expanding to devices\n",
      "that people carry around and where the very notion of location and context\n",
      "awareness is becoming increasingly important, it may be best to actually expose\n",
      "distribution rather than trying to hide it. An obvious example is making use of\n",
      "location-based services, which can often be found on mobile phones, such as\n",
      "finding a nearest shop or any nearby friends.\n",
      "RELEVANCE:\n",
      "{\"score\": \"no\"}\n",
      "<end_assessment_4>\n",
      "\n",
      "<start_assessment_5>\n",
      "QUERY:\n",
      "Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "RETRIEVED TEXT:\n",
      "Several researchers have argued that hiding distribution will lead to only\n",
      "further complicating the development of distributed systems, exactly for the\n",
      "reason that full distribution transparency can never be achieved. A popular\n",
      "technique for achieving access transparency is to extend procedure calls to\n",
      "remote servers. However, (<>)Waldo et al. [(<>)1997] already pointed out that\n",
      "attempting to hide distribution by such remote procedure calls can lead to\n",
      "poorly understood semantics, for the simple reason that a procedure call does\n",
      "change when executed over a faulty communication link.\n",
      "RELEVANCE:\n",
      "{\"score\": \"yes\"}\n",
      "<end_assessment_5>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(retrieved_documents):\n",
    "    print(f\"<start_assessment_{i+1}>\")\n",
    "    print(f\"QUERY:\\n{q}\")\n",
    "    print(\"RETRIEVED TEXT:\")\n",
    "    print_wrapped(text['text'])\n",
    "    grad = grade_retreival(q, text['text'])\n",
    "    output_text = tokenizer.decode(grad, skip_special_tokens=True)\n",
    "    print(f\"RELEVANCE:\\n{output_text}\")\n",
    "    print(f\"<end_assessment_{i+1}>\\n\")\n",
    "\n",
    "    \n",
    "#grad = grade_retreival(q, chunks_with_embeddings[0]['text'])\n",
    "\n",
    "#output_text = tokenizer.decode(grad, skip_special_tokens=True)\n",
    "#print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
      "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
      "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(SYS_PROMPT[\"relevance\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
