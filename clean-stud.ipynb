{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "with open('questions.md', 'r') as file:\n",
    "    query_list = file.read().splitlines()\n",
    "\n",
    "CONFIG = {\n",
    "    'csv_path': './data/text_chunks_with_embeddings.csv',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'attn_implementation': 'sdpa',\n",
    "    'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'embedding_model_id': 'all-mpnet-base-v2',\n",
    "}\n",
    "\n",
    "SYS_PROMPT = {\n",
    "    \"education\": \"\"\"\n",
    "    You are Study-Buddy. An educational chatbot that will aid students in their studies.\n",
    "    You are given the extracted parts of curriculum specific documents and a question. Provide a conversational and educational answer with good and easily read formatting.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
    "    \"\"\",\n",
    "    \"relevance\": \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination. \n",
    "    \"\"\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_chunks_with_embeddings(csv_path: str):\n",
    "    \"\"\"\n",
    "    Imports the chunks with embeddings from a csv file.\n",
    "    \"\"\"\n",
    "    text_chunks_with_embeddings_df = pd.read_csv(csv_path, index_col=0)\n",
    "    text_chunks_with_embeddings_df['embedding'] = text_chunks_with_embeddings_df['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "    chunks_with_embeddings = text_chunks_with_embeddings_df.to_dict(orient='records')\n",
    "    return chunks_with_embeddings\n",
    "\n",
    "def get_chunks_embeddings_as_tensor(chunks_with_embeddings: list[dict]):\n",
    "    \"\"\"\n",
    "    Converts the embeddings of chunks to a tensor.\n",
    "    \"\"\"\n",
    "    embeddings_list = [chunk['embedding'] for chunk in chunks_with_embeddings]\n",
    "    embeddings = torch.tensor(np.stack(embeddings_list, axis=0), dtype=torch.float32)\n",
    "    # embeddings = torch.tensor(np.stack(chunks_with_embeddings['embedding'].tolist(), axis=0), dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "# Load chunks and embeddings\n",
    "chunks_with_embeddings = import_chunks_with_embeddings(CONFIG['csv_path'])\n",
    "embeddings = get_chunks_embeddings_as_tensor(chunks_with_embeddings).to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                embedding_model: SentenceTransformer,\n",
    "                                n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Get dot product scores on embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "    \n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def generate_model_response(prompt: str, tokenizer, model, terminators, device=\"cuda\"):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=1024, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response)\n",
    "\n",
    "\n",
    "def get_user_prompt(query: str, retrieved_documents: list[dict]):\n",
    "    \"\"\"\n",
    "    Formats the prompt with the query and the retreived documents.\n",
    "    \"\"\"\n",
    "    base_prompt = f\"Query: {query}\\nContext:\"\n",
    "    for item in retrieved_documents:\n",
    "        base_prompt += f\"\\n- {item['text']}\"\n",
    "    return base_prompt\n",
    "\n",
    "def format_prompt(formatted_prompt: str):\n",
    "    message = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT['education'] },\n",
    "        { \"role\": \"user\", \"content\": formatted_prompt }\n",
    "    ]\n",
    "    return message\n",
    "\n",
    "\n",
    "# Load models\n",
    "embedding_model = SentenceTransformer(model_name_or_path=CONFIG['embedding_model_id'], device=CONFIG['device'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=CONFIG['model_id'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=CONFIG['model_id'], \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=False, \n",
    "    attn_implementation=CONFIG['attn_implementation']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Why is achieving full distribution transparency often impractical or even undesirable?\n",
      "----\n",
      "Achieving full distribution transparency is often impractical or even undesirable because it may not be feasible or wise to pretend that we can achieve it. Recognizing that full distribution transparency is impossible, it may be better to make distribution explicit so that users and application developers understand the sometimes unexpected behavior of a distributed system. This approach can help users better understand the system's behavior and be prepared to deal with it. Additionally, hiding distribution may lead to poorly understood semantics and complicate the development of distributed systems.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "\n",
    "# similarity scores and indices on chunk embeddings\n",
    "scores, indices = retrieve_relevant_resources(\n",
    "    query=query, \n",
    "    embeddings=embeddings, \n",
    "    embedding_model=embedding_model)\n",
    "\n",
    "user_prompt = get_user_prompt(query=query, retrieved_documents=[chunks_with_embeddings[i] for i in indices])\n",
    "\n",
    "formatted_prompt = format_prompt(user_prompt) \n",
    "\n",
    "response = generate_model_response(formatted_prompt, tokenizer, model, terminators)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Contextual Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Why is achieving full distribution transparency often impractical or even undesirable?\"\n",
    "scores, indices = retrieve_relevant_resources(\n",
    "    query=q, \n",
    "    embeddings=embeddings, \n",
    "    embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
      "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
      "\n",
      "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n",
      "\n",
      "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "[{'role': 'system', 'content': '\\n    You are Study-Buddy. An educational chatbot that will aid students in their studies.\\n    You are given the extracted parts of curriculum specific documents and a question. Provide a conversational and educational answer with good and easily read formatting.\\n    Give yourself room to think by extracting relevant passages from the context before answering the query.\\n    Don\\'t return the thinking, only return the answer.\\n    If you don\\'t know the answer, just say \"I do not know.\" Don\\'t make up an answer.\\n    '}, {'role': 'user', 'content': 'Query: Why is achieving full distribution transparency often impractical or even undesirable?\\nContext:\\n- Although distribution transparency is generally considered preferable for any distributed system, there are situations in which blindly attempting to hide all distribution aspects from users is not a good idea. A simple example is requesting your electronic newspaper to appear in your mailbox before 7 AM local time, as usual, while you are currently at the other end of the world living in a different time zone. Your morning paper will not be the morning paper you are used to. \\n- There are other arguments against distribution transparency. Recognizing that full distribution transparency is simply impossible, we should ask ourselves whether it is even wise to pretend that we can achieve it. It may be much better to make distribution explicit so that the user and application developer are never tricked into believing that there is such a thing as transparency. The result will be that users will much better understand the (sometimes unexpected) behavior of a distributed system, and are thus much better prepared to deal with this behavior. \\n- The conclusion is that aiming for distribution transparency may be a nice goal when designing and implementing distributed systems, but that it should be considered together with other issues such as performance and comprehensibility. The price for achieving full transparency may be surprisingly high. \\n- Finally, there are situations in which it is not at all obvious that hiding distribution is a good idea. As distributed systems are expanding to devices that people carry around and where the very notion of location and context awareness is becoming increasingly important, it may be best to actually expose distribution rather than trying to hide it. An obvious example is making use of location-based services, which can often be found on mobile phones, such as finding a nearest shop or any nearby friends. \\n- Several researchers have argued that hiding distribution will lead to only further complicating the development of distributed systems, exactly for the reason that full distribution transparency can never be achieved. A popular technique for achieving access transparency is to extend procedure calls to remote servers. However, (<>)Waldo et al. [(<>)1997] already pointed out that attempting to hide distribution by such remote procedure calls can lead to poorly understood semantics, for the simple reason that a procedure call does change when executed over a faulty communication link. '}]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import print_top_results_and_scores\n",
    "\n",
    "def grade_retreival(query: str, retrieved_documents: list[dict]):\n",
    "    \"\"\"\n",
    "    Grades the retrieval of documents based on the query.\n",
    "    \"\"\"\n",
    "    #print(f\"Query: {query}\\n\")\n",
    "    #print_top_results_and_scores(scores, indices, chunks_with_embeddings)\n",
    "    base_prompt = f\"Query: {query}\\nContext:\"\n",
    "    for item in retrieved_documents:\n",
    "        base_prompt += f\"\\n- {item['text']}\"\n",
    "        \n",
    "    #print(base_prompt)\n",
    "    message = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT['relevance'] },\n",
    "        { \"role\": \"user\", \"content\": formatted_prompt }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        #return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(prompt)\n",
    "\n",
    "\n",
    "grade_retreival(q, [chunks_with_embeddings[i] for i in indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
