{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "with open('questions.md', 'r') as file:\n",
    "    query_list = file.read().splitlines()\n",
    "\n",
    "CONFIG = {\n",
    "    'csv_path': './data/text_chunks_with_embeddings.csv',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'attn_implementation': 'sdpa',\n",
    "    'model_id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'embedding_model_id': 'all-mpnet-base-v2',\n",
    "}\n",
    "\n",
    "SYS_PROMPT = {\n",
    "    \"education\": \"\"\"\n",
    "    You are Study-Buddy. An educational chatbot that will aid students in their studies.\n",
    "    You are given the extracted parts of curriculum specific documents and a question. Provide a conversational and educational answer with good and easily read formatting.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def import_chunks_with_embeddings(csv_path: str):\n",
    "    \"\"\"\n",
    "    Imports the chunks with embeddings from a csv file.\n",
    "    \"\"\"\n",
    "    text_chunks_with_embeddings_df = pd.read_csv(csv_path, index_col=0)\n",
    "    text_chunks_with_embeddings_df['embedding'] = text_chunks_with_embeddings_df['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "    chunks_with_embeddings = text_chunks_with_embeddings_df.to_dict(orient='records')\n",
    "    return chunks_with_embeddings\n",
    "\n",
    "def get_chunks_embeddings_as_tensor(chunks_with_embeddings: list[dict]):\n",
    "    \"\"\"\n",
    "    Converts the embeddings of chunks to a tensor.\n",
    "    \"\"\"\n",
    "    embeddings_list = [chunk['embedding'] for chunk in chunks_with_embeddings]\n",
    "    embeddings = torch.tensor(np.stack(embeddings_list, axis=0), dtype=torch.float32)\n",
    "    # embeddings = torch.tensor(np.stack(chunks_with_embeddings['embedding'].tolist(), axis=0), dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                embedding_model: SentenceTransformer,\n",
    "                                n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "    # Embed query\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Get dot product scores on embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "    \n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def generate_model_response(prompt: str, tokenizer, model, terminators, device=\"cuda\"):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=1024, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response)\n",
    "\n",
    "\n",
    "def get_user_prompt(query: str, retrieved_documents: list[dict]):\n",
    "    \"\"\"\n",
    "    Formats the prompt with the query and the retreived documents.\n",
    "    \"\"\"\n",
    "    base_prompt = f\"Query: {query}\\nContext:\"\n",
    "    for item in retrieved_documents:\n",
    "        base_prompt += f\"\\n- {item['text']}\"\n",
    "    return base_prompt\n",
    "\n",
    "def format_prompt(formatted_prompt: str):\n",
    "    message = [\n",
    "        { \"role\": \"system\", \"content\": SYS_PROMPT['education'] },\n",
    "        { \"role\": \"user\", \"content\": formatted_prompt }\n",
    "    ]\n",
    "    return message\n",
    "    \n",
    "def generate_response(query: str, embeddings: torch.tensor, embedding_model: SentenceTransformer, tokenizer, model, terminators, chunks_with_embeddings: list[dict]):\n",
    "    scores, indices = retrieve_relevant_resources(\n",
    "        query=query, \n",
    "        embeddings=embeddings, \n",
    "        embedding_model=embedding_model)\n",
    "    user_prompt = get_user_prompt(query=query, retrieved_documents=[chunks_with_embeddings[i] for i in indices])\n",
    "    formatted_prompt = format_prompt(user_prompt) \n",
    "    return generate_model_response(formatted_prompt, tokenizer, model, terminators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_embeddings = import_chunks_with_embeddings(CONFIG['csv_path'])\n",
    "embeddings = get_chunks_embeddings_as_tensor(chunks_with_embeddings).to(CONFIG['device'])\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=CONFIG['embedding_model_id'], device=CONFIG['device'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=CONFIG['model_id'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=CONFIG['model_id'], \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=False, \n",
    "    attn_implementation=CONFIG['attn_implementation']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the different threading models (many-to-one, one-to-one, many-to-many) and when is each model most appropriate?\n",
      "----\n",
      "Let's dive into the different threading models!\n",
      "\n",
      "There are three main threading models:\n",
      "\n",
      "**Many-to-One (Single-Threaded Process)**\n",
      "In this model, a single thread is used to handle multiple requests. This means that only one request can be processed at a time, making it a sequential process. This model is suitable when:\n",
      "\n",
      "* You don't need to prioritize requests\n",
      "* You don't have a high volume of requests\n",
      "* You want to keep things simple\n",
      "\n",
      "**One-to-One (Finite-State Machine)**\n",
      "In this model, each request is handled by a separate thread. This allows for true parallelism, but it requires non-blocking system calls, which can be challenging to program and maintain. This model is suitable when:\n",
      "\n",
      "* You need to prioritize requests\n",
      "* You have a high volume of requests\n",
      "* You're willing to invest time in programming and maintaining non-blocking system calls\n",
      "\n",
      "**Many-to-Many (Hybrid Threading Model)**\n",
      "In this model, multiple user-level threads can be mapped to multiple kernel-level threads. This allows for parallelism and the use of blocking system calls, making it a more practical solution. This model is suitable when:\n",
      "\n",
      "* You need to balance parallelism and ease of programming\n",
      "* You want to use blocking system calls\n",
      "* You need to protect objects against concurrent access\n",
      "\n",
      "In the many-to-many model, each kernel thread can run its own user-level thread, and the thread package can be shared by multiple kernel threads. This approach ensures that objects are automatically protected against concurrent access.\n",
      "\n",
      "When choosing a threading model, consider factors such as the volume of requests, priority of requests, and the level of parallelism required. There is no single best policy, and the choice ultimately depends on your specific use case.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "response = generate_response(query, embeddings, embedding_model, tokenizer, model, terminators, chunks_with_embeddings)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"----\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
