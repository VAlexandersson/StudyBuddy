{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pymupdf4llm\n",
    "\n",
    "pdf_path = \"./data/Distributed_Systems_4.pdf\"\n",
    "\n",
    "doc = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n",
    "\n",
    "import fitz\n",
    "\n",
    "if fitz.pymupdf_version_tuple < (1, 24, 0):\n",
    "    raise NotImplementedError(\"PyMuPDF version 1.24.0 or later is needed.\")\n",
    "\n",
    "\n",
    "def to_markdown(doc: fitz.Document, pages: list = None) -> str:\n",
    "    \"\"\"Process the document and return the text of its selected pages.\"\"\"\n",
    "    if isinstance(doc, str):\n",
    "        doc = fitz.open(doc)\n",
    "    SPACES = set(string.whitespace)  # used to check relevance of text pieces\n",
    "    if not pages:  # use all pages if argument not given\n",
    "        pages = range(doc.page_count)\n",
    "\n",
    "    class IdentifyHeaders:\n",
    "        \"\"\"Compute data for identifying header text.\"\"\"\n",
    "\n",
    "        def __init__(self, doc, pages: list = None, body_limit: float = None):\n",
    "            \"\"\"Read all text and make a dictionary of fontsizes.\n",
    "\n",
    "            Args:\n",
    "                pages: optional list of pages to consider\n",
    "                body_limit: consider text with larger font size as some header\n",
    "            \"\"\"\n",
    "            if pages is None:  # use all pages if omitted\n",
    "                pages = range(doc.page_count)\n",
    "                \n",
    "            fontsizes = {}\n",
    "            for pno in pages:\n",
    "                page = doc[pno]\n",
    "                blocks = page.get_text(\"dict\", flags=fitz.TEXTFLAGS_TEXT)[\"blocks\"]\n",
    "                for span in [  # look at all non-empty horizontal spans\n",
    "                    s\n",
    "                    for b in blocks\n",
    "                    for l in b[\"lines\"]\n",
    "                    for s in l[\"spans\"]\n",
    "                    if not SPACES.issuperset(s[\"text\"])\n",
    "                ]:\n",
    "                    fontsz = round(span[\"size\"])\n",
    "                    count = fontsizes.get(fontsz, 0) + len(span[\"text\"].strip())\n",
    "                    fontsizes[fontsz] = count\n",
    "\n",
    "            # maps a fontsize to a string of multiple # header tag characters\n",
    "            self.header_id = {}\n",
    "\n",
    "            # If not provided, choose the most frequent font size as body text.\n",
    "            # If no text at all on all pages, just use 12\n",
    "            if body_limit is None:\n",
    "                temp = sorted(\n",
    "                    [(k, v) for k, v in fontsizes.items()],\n",
    "                    key=lambda i: i[1],\n",
    "                    reverse=True,\n",
    "                )\n",
    "                if temp:\n",
    "                    body_limit = temp[0][0]\n",
    "                else:\n",
    "                    body_limit = 12\n",
    "\n",
    "            sizes = sorted(\n",
    "                [f for f in fontsizes.keys() if f > body_limit], reverse=True\n",
    "            )\n",
    "\n",
    "            # make the header tag dictionary\n",
    "            for i, size in enumerate(sizes):\n",
    "                self.header_id[size] = \"#\" * (i + 1) + \" \"\n",
    "\n",
    "        def get_header_id(self, span, line_spans):\n",
    "            \"\"\"Return appropriate markdown header prefix.\n",
    "\n",
    "            Given a text span from a \"dict\"/\"radict\" extraction, determine the\n",
    "            markdown header prefix string of 0 to many concatenated '#' characters.\n",
    "            \"\"\"\n",
    "            fontsize = round(span[\"size\"])  # compute fontsize\n",
    "            hdr_id = self.header_id.get(fontsize, \"\")\n",
    "\n",
    "            bold = span[\"flags\"] & 16\n",
    "            standalone = len(line_spans) == 1 and not SPACES.issuperset(span[\"text\"])\n",
    "            if bold and standalone and not hdr_id:\n",
    "                # If the span is bold and standalone, consider it as a heading\n",
    "                if hdr_id == \"\":\n",
    "                    smallest_heading = max(self.header_id.values(), default=\"\")\n",
    "                    hdr_id = \"#\" * (len(smallest_heading.strip()) + 1) + \" \"\n",
    "\n",
    "                \n",
    "            return hdr_id\n",
    "\n",
    "    def resolve_links(links, span):\n",
    "        \"\"\"Accept a span bbox and return a markdown link string.\"\"\"\n",
    "        bbox = fitz.Rect(span[\"bbox\"])  # span bbox\n",
    "        # a link should overlap at least 70% of the span\n",
    "        bbox_area = 0.7 * abs(bbox)\n",
    "        for link in links:\n",
    "            hot = link[\"from\"]  # the hot area of the link\n",
    "            if not abs(hot & bbox) >= bbox_area:\n",
    "                continue  # does not touch the bbox\n",
    "            text = f'[{span[\"text\"].strip()}]({link[\"uri\"]})'\n",
    "            return text\n",
    "\n",
    "    def write_text(page, clip, hdr_prefix):\n",
    "        \"\"\"Output the text found inside the given clip.\n",
    "\n",
    "        This is an alternative for plain text in that it outputs\n",
    "        text enriched with markdown styling.\n",
    "        The logic is capable of recognizing headers, body text, code blocks,\n",
    "        inline code, bold, italic and bold-italic styling.\n",
    "        There is also some effort for list supported (ordered / unordered) in\n",
    "        that typical characters are replaced by respective markdown characters.\n",
    "        \"\"\"\n",
    "        out_string = \"\"\n",
    "        code = False  # mode indicator: outputting code\n",
    "\n",
    "        # extract URL type links on page\n",
    "        links = [l for l in page.get_links() if l[\"kind\"] == 2]\n",
    "\n",
    "        blocks = page.get_text(\n",
    "            \"dict\",\n",
    "            clip=clip,\n",
    "            flags=fitz.TEXTFLAGS_TEXT,\n",
    "            sort=True,\n",
    "        )[\"blocks\"]\n",
    "\n",
    "        for block in blocks:  # iterate textblocks\n",
    "            previous_y = 0\n",
    "            for line in block[\"lines\"]:  # iterate lines in block\n",
    "                if line[\"dir\"][1] != 0:  # only consider horizontal lines\n",
    "                    continue\n",
    "                spans = [s for s in line[\"spans\"]]\n",
    "\n",
    "                this_y = line[\"bbox\"][3]  # current bottom coord\n",
    "\n",
    "                # check for still being on same line\n",
    "                same_line = abs(this_y - previous_y) <= 3 and previous_y > 0\n",
    "\n",
    "                if same_line and out_string.endswith(\"\\n\"):\n",
    "                    out_string = out_string[:-1]\n",
    "\n",
    "                # are all spans in line in a mono-spaced font?\n",
    "                all_mono = all([s[\"flags\"] & 8 for s in spans])\n",
    "\n",
    "                # compute text of the line\n",
    "                text = \"\".join([s[\"text\"] for s in spans])\n",
    "                if not same_line:\n",
    "                    previous_y = this_y\n",
    "                    if not out_string.endswith(\"\\n\"):\n",
    "                        out_string += \"\\n\"\n",
    "\n",
    "                if all_mono:\n",
    "                    # compute approx. distance from left - assuming a width\n",
    "                    # of 0.5*fontsize.\n",
    "                    delta = int(\n",
    "                        (spans[0][\"bbox\"][0] - block[\"bbox\"][0])\n",
    "                        / (spans[0][\"size\"] * 0.5)\n",
    "                    )\n",
    "                    if not code:  # if not already in code output  mode:\n",
    "                        out_string += \"```\"  # switch on \"code\" mode\n",
    "                        code = True\n",
    "                    if not same_line:  # new code line with left indentation\n",
    "                        out_string += \"\\n\" + \" \" * delta + text + \" \"\n",
    "                        previous_y = this_y\n",
    "                    else:  # same line, simply append\n",
    "                        out_string += text + \" \"\n",
    "                    continue  # done with this line\n",
    "\n",
    "                for i, s in enumerate(spans):  # iterate spans of the line\n",
    "                    # this line is not all-mono, so switch off \"code\" mode\n",
    "                    if code:  # still in code output mode?\n",
    "                        out_string += \"```\\n\"  # switch of code mode\n",
    "                        code = False\n",
    "                    # decode font properties\n",
    "                    mono = s[\"flags\"] & 8\n",
    "                    bold = s[\"flags\"] & 16\n",
    "                    italic = s[\"flags\"] & 2\n",
    "\n",
    "                    if mono:\n",
    "                        # this is text in some monospaced font\n",
    "                        out_string += f\"`{s['text'].strip()}` \"\n",
    "                    else:  # not a mono text\n",
    "                        # for first span, get header prefix string if present\n",
    "                        if i == 0:\n",
    "                            hdr_string = hdr_prefix.get_header_id(s, line[\"spans\"])\n",
    "                        else:\n",
    "                            hdr_string = \"\"\n",
    "                        prefix = \"\"\n",
    "                        suffix = \"\"\n",
    "                        if hdr_string == \"\":\n",
    "                            if bold:\n",
    "                                prefix = \"**\"\n",
    "                                suffix += \"**\"\n",
    "                            if italic:\n",
    "                                prefix += \"_\"\n",
    "                                suffix = \"_\" + suffix\n",
    "\n",
    "                        ltext = resolve_links(links, s)\n",
    "                        if ltext:\n",
    "                            text = f\"{hdr_string}{prefix}{ltext}{suffix} \"\n",
    "                        else:\n",
    "                            text = f\"{hdr_string}{prefix}{s['text'].strip()}{suffix} \"\n",
    "                        text = (\n",
    "                            text.replace(\"<\", \"&lt;\")\n",
    "                            .replace(\">\", \"&gt;\")\n",
    "                            .replace(chr(0xF0B7), \"-\")\n",
    "                            .replace(chr(0xB7), \"-\")\n",
    "                            .replace(chr(8226), \"-\")\n",
    "                            .replace(chr(9679), \"-\")\n",
    "                        )\n",
    "                        out_string += text\n",
    "                previous_y = this_y\n",
    "                if not code:\n",
    "                    out_string += \"\\n\"\n",
    "            out_string += \"\\n\"\n",
    "        if code:\n",
    "            out_string += \"```\\n\"  # switch of code mode\n",
    "            code = False\n",
    "        return out_string.replace(\" \\n\", \"\\n\")\n",
    "\n",
    "    hdr_prefix = IdentifyHeaders(doc, pages=pages)\n",
    "    md_string = \"\"\n",
    "    md_pages = []  # list to hold text and metadata\n",
    "    for pno in tqdm(pages):\n",
    "        page = doc[pno]\n",
    "        # 1. first locate all tables on page\n",
    "        tabs = page.find_tables()\n",
    "        md_page = \"\"  # string to hold markdown for this page\n",
    "\n",
    "        # 2. make a list of table boundary boxes, sort by top-left corner.\n",
    "        # Must include the header bbox, which may be external.\n",
    "        tab_rects = sorted(\n",
    "            [\n",
    "                (fitz.Rect(t.bbox) | fitz.Rect(t.header.bbox), i)\n",
    "                for i, t in enumerate(tabs.tables)\n",
    "            ],\n",
    "            key=lambda r: (r[0].y0, r[0].x0),\n",
    "        )\n",
    "\n",
    "        # 3. final list of all text and table rectangles\n",
    "        text_rects = []\n",
    "        # compute rectangles outside tables and fill final rect list\n",
    "        for i, (r, idx) in enumerate(tab_rects):\n",
    "            if i == 0:  # compute rect above all tables\n",
    "                tr = page.rect\n",
    "                tr.y1 = r.y0\n",
    "                if not tr.is_empty:\n",
    "                    text_rects.append((\"text\", tr, 0))\n",
    "                text_rects.append((\"table\", r, idx))\n",
    "                continue\n",
    "            # read previous rectangle in final list: always a table!\n",
    "            _, r0, idx0 = text_rects[-1]\n",
    "\n",
    "            # check if a non-empty text rect is fitting in between tables\n",
    "            tr = page.rect\n",
    "            tr.y0 = r0.y1\n",
    "            tr.y1 = r.y0\n",
    "            if not tr.is_empty:  # empty if two tables overlap vertically!\n",
    "                text_rects.append((\"text\", tr, 0))\n",
    "\n",
    "            text_rects.append((\"table\", r, idx))\n",
    "\n",
    "            # there may also be text below all tables\n",
    "            if i == len(tab_rects) - 1:\n",
    "                tr = page.rect\n",
    "                tr.y0 = r.y1\n",
    "                if not tr.is_empty:\n",
    "                    text_rects.append((\"text\", tr, 0))\n",
    "\n",
    "        if not text_rects:  # this will happen for table-free pages\n",
    "            text_rects.append((\"text\", page.rect, 0))\n",
    "        else:\n",
    "            rtype, r, idx = text_rects[-1]\n",
    "            if rtype == \"table\":\n",
    "                tr = page.rect\n",
    "                tr.y0 = r.y1\n",
    "                if not tr.is_empty:\n",
    "                    text_rects.append((\"text\", tr, 0))\n",
    "\n",
    "        # we have all rectangles and can start outputting their contents\n",
    "        for rtype, r, idx in text_rects:\n",
    "            if rtype == \"text\":  # a text rectangle\n",
    "                text = write_text(page, r, hdr_prefix)# write MD content\n",
    "                text += \"\\n\"\n",
    "                md_page += text  \n",
    "                md_string += text  # write MD content\n",
    "            else:  # a table rect\n",
    "                md_tab = tabs[idx].to_markdown(clean=False)\n",
    "                \n",
    "                md_page += md_tab\n",
    "                md_string += md_tab\n",
    "            \n",
    "        md_string += \"\\n-----\\n\\n\"\n",
    "        \n",
    "        # Split the page into blocks for further processing\n",
    "        blocks = md_page.split(\"\\n\\n\")\n",
    "        for i, block in enumerate(blocks):\n",
    "            text = block.replace(\"-\\n\", \"\")\n",
    "            text = block.replace(\"-**\\n**\", \"\")\n",
    "            \n",
    "             \n",
    "            text = text.replace(\"\\n\", \" \").strip()\n",
    "            \n",
    "            if text == \"\":\n",
    "                blocks.pop(i)\n",
    "            else:\n",
    "                blocks[i] = text\n",
    "        md_pages.append({\n",
    "            \"document\": md_page, \n",
    "            \"page_number\": pno+1, \n",
    "            \"blocks\": blocks\n",
    "            })  \n",
    "    return md_pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def similar(a: str, b: str, matcher=SequenceMatcher(None, '', '')) -> float:\n",
    "    matcher.set_seqs(a, b)\n",
    "    return matcher.ratio()\n",
    "\n",
    "\n",
    "def identify_potential_headers_or_footers(blocks: List[str], threshold: float = 0.8) -> List[Tuple[int, List[int]]]:\n",
    "    potential_blocks = []\n",
    "    for i, block in tqdm(enumerate(blocks)):\n",
    "        similarities = [similar(block, other_block) for other_block in blocks]\n",
    "        similar_indices = [j for j, sim in enumerate(similarities) if sim > threshold]\n",
    "        if len(similar_indices) > 1:\n",
    "            potential_blocks.append((i, similar_indices))\n",
    "    return potential_blocks\n",
    "\n",
    "def has_header_or_footer(num_pages: int, potential_blocks: List[Tuple[int, List[int]]]) -> List[bool]:\n",
    "    block_indices = {index for index, _ in potential_blocks}\n",
    "    return [i in block_indices for i in range(num_pages)]\n",
    "\n",
    "def remove_headers_or_footers(md_p: List[Dict], header: bool, has_block: List[bool]) -> List[Dict]:\n",
    "    for page, has_header_or_footer in zip(md_p, has_block):\n",
    "        if has_header_or_footer:\n",
    "            page[\"blocks\"] = page[\"blocks\"][1:] if header else page[\"blocks\"][:-1]\n",
    "    return md_p\n",
    "\n",
    "def remove_headers_and_footers(md_p: List[Dict]) -> List[Dict]:\n",
    "    for header in [True, False]:\n",
    "        print(\"Removing potential headers\") if header else print(\"Removing potential footers\")\n",
    "        blocks = [page[\"blocks\"][0 if header else -1] for page in md_p if page[\"blocks\"]]\n",
    "        potential_blocks = identify_potential_headers_or_footers(blocks)\n",
    "        has_block = has_header_or_footer(len(md_p), potential_blocks)\n",
    "        md_p = remove_headers_or_footers(md_p, header, has_block)\n",
    "    return md_p\n",
    "\n",
    "def find_entities_by_page(paragraphs: List[Dict], page_number: int) -> List[Dict]:\n",
    "    return [entity for entity in paragraphs if entity.get('pagenumber') == page_number]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Cleaned Pages\n",
    "1. Get markdown text\n",
    "2. Remove headers and footers.\n",
    "3. Store whole pages in a db with a uid for traceable chunking.\n",
    "4. Split pages into blocks \n",
    "    \n",
    "    4.1 TODO: Store them with a unique ids and the parent page ID\n",
    "5. Split block into sentences \n",
    "    \n",
    "    5.1 TODO: Store sentences with a ID and its parent block id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01083bfb285146e582554785e1fed6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing potential headers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe283f4c54c45a2bf12f449b1444e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing potential footers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29400042f9d413ea6549f48e9e30309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_page = 16\n",
    "end_page = 632\n",
    "doc = fitz.open(pdf_path)  # open input file\n",
    "pages = range(start_page, end_page) # get page range\n",
    "\n",
    "md_pages = to_markdown(doc, pages=pages) # get markdown paragraphs as list of pages\n",
    "md_pages = remove_headers_and_footers(md_pages) # remove potential headers and footers from pages\n",
    "\n",
    "import pandas as pd\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buddy/Study-Buddy/env/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def create_blocks_df(md_pages_df):\n",
    "    blocks = []\n",
    "    for _, row in md_pages_df.iterrows():\n",
    "        for block in row['blocks']:\n",
    "            blocks.append({\n",
    "                \"id\": uuid.uuid4(),\n",
    "                \"order_id\": len(blocks), \n",
    "                \"text\": block, \n",
    "                \"page_number\": row[\"page_number\"], \n",
    "                \"page_id\": row[\"id\"]\n",
    "            })\n",
    "\n",
    "    blocks_df = pd.DataFrame(blocks)\n",
    "    return blocks_df\n",
    "\n",
    "def create_sentences_df(blocks_df):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    sentences = []\n",
    "    for _, row in blocks_df.iterrows():\n",
    "        doc = nlp(row['text'])\n",
    "        for sent in doc.sents:\n",
    "            sentences.append({\n",
    "                \"id\": uuid.uuid4(),\n",
    "                \"order_id\": len(sentences),\n",
    "                \"text\": sent.text, \n",
    "                \"block_id\": row[\"id\"]})\n",
    "\n",
    "    sentences_df = pd.DataFrame(sentences)\n",
    "    return sentences_df\n",
    "\n",
    "md_pages_df = pd.DataFrame(md_pages)\n",
    "\n",
    "md_pages_df['order_id'] = range(1, len(md_pages) + 1)\n",
    "md_pages_df['id'] = [uuid.uuid4() for _ in range(len(md_pages))]\n",
    "\n",
    "blocks_df = create_blocks_df(md_pages_df)\n",
    "md_pages_df.drop(columns=['blocks'], inplace=True)\n",
    "sentences_df = create_sentences_df(blocks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_df['num_sentences'] = blocks_df['block'].apply(lambda x: x.count('.') + x.count('!') + x.count('?'))\n",
    "blocks_df['num_words'] = blocks_df['block'].apply(lambda x: len(x.split()))\n",
    "blocks_df['num_chars'] = blocks_df['block'].apply(len)\n",
    "blocks_df['token_count'] = blocks_df['num_chars'].apply(lambda x: round(x/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Study-Buddy/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m blocks_df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m blocks_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m         current_heading \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# update the current heading\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     blocks_df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m current_heading  \u001b[38;5;66;03m# assign the current heading\u001b[39;00m\n",
      "File \u001b[0;32m~/Study-Buddy/env/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Study-Buddy/env/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Study-Buddy/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentence'"
     ]
    }
   ],
   "source": [
    "blocks_df.head()\n",
    "for idx, row in blocks_df.iterrows():\n",
    "    if row['sentence'].startswith('#'):\n",
    "        current_heading = row['sentence']  # update the current heading\n",
    "    blocks_df.at[idx, 'heading'] = current_heading  # assign the current heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All good above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df[(df[\"block\"].str.startswith((\"#\", \"-\")) | df[\"block\"].str.match(r\"^\\d+\\.\")) | (df[\"num_chars\"] >= 10 )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3740717/539341223.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_f['heading'] = df_f['heading'].str.replace('#', '')\n",
      "/tmp/ipykernel_3740717/539341223.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_f['parent_heading'] = df_f['parent_heading'].str.replace('#', '')\n"
     ]
    }
   ],
   "source": [
    "df_f = df_f.rename(columns={\"pagenumber\": \"page\"})\n",
    "df_f = df_f[~df_f[\"block\"].str.startswith(\"#\")]\n",
    "\n",
    "df_f = df_f[(df_f[\"num_chars\"] >= 15 )]\n",
    "df_f = df_f[(df_f[\"block\"].str.startswith((\"-\")) | (df_f[\"num_chars\"] >= 20 ))]\n",
    "df_f = df_f[(df_f[\"block\"].str.startswith((\"**\")) | df_f[\"block\"].str.match(r\"^\\d+\\.\")) | (df_f[\"token_count\"] >= 30 )]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_f = df[(df[\"block\"].str.startswith((\"#\", \"-\", \"**\")) | df[\"block\"].str.match(r\"^\\d+\\.\")) | (df[\"num_words\"] >= 20)]\n",
    "df_f = df[(df[\"block\"].str.startswith((\"#\", \"-\")) | df[\"block\"].str.match(r\"^\\d+\\.\")) | (df[\"num_chars\"] >= 10 )]\n",
    "\n",
    "df_sentence = df.explode(\"sentences\")\n",
    "df_sentence = df_sentence.rename(columns={\"sentences\": \"sentence\"})\n",
    "df_sentence = df_sentence.rename(columns={\"pagenumber\": \"page\"})\n",
    "df_sentence = df_sentence[[\"id\", \"page\", \"sentence\"]]\n",
    "df_sentence['num_words'] = df_sentence['sentence'].str.split().str.len() # df_sentence['num_words'] = df_sentence['sentence'].apply(lambda x: len(x.split()))\n",
    "df_sentence['num_chars'] = df_sentence['sentence'].str.len() # df_sentence['num_chars'] = df_sentence['sentence'].apply(len)\n",
    "df_sentence['token_count'] = (df_sentence['num_chars']/4).round() # df_sentence['token_count'] = df_sentence['num_chars'].apply(lambda x: round(x/4))\n",
    "df_sentence['heading'] = None  # create a new column for headings\n",
    "current_heading = None  # variable to hold the current heading\n",
    "\n",
    "for idx, row in df_sentence.iterrows():\n",
    "    if row['sentence'].startswith('#'):\n",
    "        current_heading = row['sentence']  # update the current heading\n",
    "    df_sentence.at[idx, 'heading'] = current_heading  # assign the current heading\n",
    "\n",
    "df_sentence['heading'] = df_sentence['heading'].str.replace('#', '')\n",
    "\n",
    "df_sentence = df_sentence[~df_sentence[\"sentence\"].str.startswith(\"#\")]\n",
    "\n",
    "df_sentence = df_sentence[(df_sentence[\"num_chars\"] >= 15 )]\n",
    "df_sentence = df_sentence[(df_sentence[\"sentence\"].str.startswith((\"-\")) | (df_sentence[\"num_chars\"] >= 20 ))]\n",
    "df_sentence = df_sentence[(df_sentence[\"sentence\"].str.startswith((\"**\")) | df_sentence[\"sentence\"].str.match(r\"^\\d+\\.\")) | (df_sentence[\"num_chars\"] >= 30 )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 5\n",
    "df_chunk = df_sentence[[\"id\", \"page\", \"sentence\", \"heading\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3740717/3353215871.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_chunk['group_id'] = (df_chunk['heading'].ne(df_chunk['heading'].shift()) | df_chunk['page'].ne(df_chunk['page'].shift())).cumsum()\n",
      "/tmp/ipykernel_3740717/3353215871.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_chunk['chunked_sentences'] = df_chunk.groupby('group_id').apply(chunk_sentences)\n",
      "/tmp/ipykernel_3740717/3353215871.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_chunk['chunked_sentences'] = df_chunk.groupby('group_id').apply(chunk_sentences)\n"
     ]
    }
   ],
   "source": [
    "df_chunk['group_id'] = (df_chunk['heading'].ne(df_chunk['heading'].shift()) | df_chunk['page'].ne(df_chunk['page'].shift())).cumsum()\n",
    "def chunk_sentences(group):\n",
    "    return group['sentence'].tolist()\n",
    "df_chunk['chunked_sentences'] = df_chunk.groupby('group_id').apply(chunk_sentences)\n",
    "df_chunk = df_chunk[df_chunk['chunked_sentences'].str.len() > 0]\n",
    "df_chunk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = df_chunk.groupby('group_id').agg({\n",
    "    'heading': 'first',\n",
    "    'page': 'first',\n",
    "    'chunked_sentences': 'sum'\n",
    "})\n",
    "\n",
    "# Reset the index\n",
    "df_chunk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14595.00</td>\n",
       "      <td>14595.00</td>\n",
       "      <td>14595.00</td>\n",
       "      <td>14595.00</td>\n",
       "      <td>14595.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2438.20</td>\n",
       "      <td>357.85</td>\n",
       "      <td>19.05</td>\n",
       "      <td>111.46</td>\n",
       "      <td>27.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1299.06</td>\n",
       "      <td>198.11</td>\n",
       "      <td>9.70</td>\n",
       "      <td>54.64</td>\n",
       "      <td>13.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1270.00</td>\n",
       "      <td>188.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2536.00</td>\n",
       "      <td>362.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3494.00</td>\n",
       "      <td>530.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>143.00</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5002.00</td>\n",
       "      <td>685.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>906.00</td>\n",
       "      <td>226.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      page  num_words  num_chars  token_count\n",
       "count  14595.00  14595.00   14595.00   14595.00     14595.00\n",
       "mean    2438.20    357.85      19.05     111.46        27.87\n",
       "std     1299.06    198.11       9.70      54.64        13.66\n",
       "min        7.00      5.00       1.00      22.00         6.00\n",
       "25%     1270.00    188.00      12.00      70.00        18.00\n",
       "50%     2536.00    362.00      18.00     103.00        26.00\n",
       "75%     3494.00    530.00      24.00     143.00        36.00\n",
       "max     5002.00    685.00     180.00     906.00       226.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence['structure_type'] = None  # create a new column for headings\n",
    "\n",
    "df_sentence.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3861, 217, 3020)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df[df[\"block\"].str.startswith(\"#\")]), len(df[~df[\"block\"].str.startswith((\"#\", \"**\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df[(df[\"block\"].str.startswith((\"#\", \"-\", \"**\")) | df[\"block\"].str.match(r\"^\\d+\\.\")) | (df[\"num_words\"] >= 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>686.00</td>\n",
       "      <td>686.00</td>\n",
       "      <td>686.00</td>\n",
       "      <td>686.00</td>\n",
       "      <td>686.00</td>\n",
       "      <td>686.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>342.50</td>\n",
       "      <td>343.50</td>\n",
       "      <td>2592.11</td>\n",
       "      <td>428.92</td>\n",
       "      <td>30.45</td>\n",
       "      <td>648.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>198.18</td>\n",
       "      <td>198.18</td>\n",
       "      <td>712.89</td>\n",
       "      <td>132.26</td>\n",
       "      <td>64.93</td>\n",
       "      <td>178.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>171.25</td>\n",
       "      <td>172.25</td>\n",
       "      <td>2285.75</td>\n",
       "      <td>367.25</td>\n",
       "      <td>18.00</td>\n",
       "      <td>571.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>342.50</td>\n",
       "      <td>343.50</td>\n",
       "      <td>2782.50</td>\n",
       "      <td>452.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>695.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>513.75</td>\n",
       "      <td>514.75</td>\n",
       "      <td>3093.25</td>\n",
       "      <td>503.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>773.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>685.00</td>\n",
       "      <td>686.00</td>\n",
       "      <td>3724.00</td>\n",
       "      <td>1033.00</td>\n",
       "      <td>819.00</td>\n",
       "      <td>931.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    page  char_count  word_count  sentence_count  token_count\n",
       "count  686.00  686.00      686.00      686.00          686.00       686.00\n",
       "mean   342.50  343.50     2592.11      428.92           30.45       648.03\n",
       "std    198.18  198.18      712.89      132.26           64.93       178.22\n",
       "min      0.00    1.00        0.00        0.00            1.00         0.00\n",
       "25%    171.25  172.25     2285.75      367.25           18.00       571.44\n",
       "50%    342.50  343.50     2782.50      452.00           23.00       695.62\n",
       "75%    513.75  514.75     3093.25      503.00           26.00       773.31\n",
       "max    685.00  686.00     3724.00     1033.00          819.00       931.00"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "page             int64\n",
       "sentence        object\n",
       "num_words        int64\n",
       "num_chars        int64\n",
       "token_count    float64\n",
       "heading         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import string\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def to_markdown(doc: fitz.Document, pages: list = None):\n",
    "    \"\"\"Process the document and return the text of its selected pages with classifications.\"\"\"\n",
    "    if isinstance(doc, str):\n",
    "        doc = fitz.open(doc)\n",
    "    SPACES = set(string.whitespace)\n",
    "    if not pages:\n",
    "        pages = range(doc.page_count)\n",
    "\n",
    "    def classify_span(span):\n",
    "        text = span[\"text\"].strip()\n",
    "        if span[\"size\"] > 14 and span[\"font\"].endswith(\"Bold\"):\n",
    "            category = \"heading\"\n",
    "        elif span[\"size\"] > 12 and not span[\"font\"].endswith(\"Bold\"):\n",
    "            category = \"subheading\"\n",
    "        elif text.startswith((\"-\", \"*\")):\n",
    "            category = \"list_item\"\n",
    "        else:\n",
    "            category = \"body\"\n",
    "        return category, \"#\" * (category.count(\"_\") + 1) + \" \"  # Header level based on underscores\n",
    "\n",
    "    def write_text(page, clip):\n",
    "        \"\"\"Output text with markdown formatting and classifications.\"\"\"\n",
    "        blocks = page.get_text(\"dict\", clip=clip, flags=fitz.TEXTFLAGS_TEXT)[\"blocks\"]\n",
    "        md_text = \"\"\n",
    "        for block in blocks:\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    category, header_prefix = classify_span(span)\n",
    "                    md_text += f\"{header_prefix}{span['text'].strip()}\\n\"\n",
    "        return md_text\n",
    "\n",
    "    md_pages = []\n",
    "    for pno in tqdm(pages):\n",
    "        page = doc[pno]\n",
    "        md_page = write_text(page, page.rect)\n",
    "        md_pages.append({\n",
    "            \"document\": md_page,\n",
    "            \"page_number\": pno + 1\n",
    "        })\n",
    "    return md_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fcef021c4e45feb0893f3501897be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md = to_markdown(doc, pages=range(200, 300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
